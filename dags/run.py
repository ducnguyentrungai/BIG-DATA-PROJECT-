# import os
# import logging
# import subprocess
# from airflow import DAG
# from airflow.operators.python import PythonOperator
# from datetime import datetime


# def run_main_pipeline():
#     script_path = '/opt/airflow/scripts/main/run_pipeline.py'
#     if not os.path.exists(script_path):
#         raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y script: {script_path}")
    
#     logging.info(f"‚ñ∂Ô∏è ƒêang ch·∫°y pipeline: {script_path}")
#     subprocess.run(["python", script_path], check=True)
#     logging.info("‚úÖ Pipeline ch·∫°y th√†nh c√¥ng")

# # C·∫•u h√¨nh DAG
# default_args = {
#     'owner': 'trungduc',
#     'retries': 1
# }

# with DAG(
#     dag_id='stock_etl_pipeline_new',
#     default_args=default_args,
#     description='Pipeline ETL c·ªï phi·∫øu: Yahoo ‚Üí Kafka ‚Üí Parquet',
#     start_date=datetime(2024, 1, 1),
#     schedule_interval=None,
#     catchup=False,
#     tags=['stock', 'etl', 'pipeline']
# ) as dag:

#     run_pipeline_task = PythonOperator(
#         task_id='run_main_pipeline',
#         python_callable=run_main_pipeline
#     )

import os
import logging
import subprocess
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime


def run_main_pipeline():
    """
    G·ªçi script Python ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô ETL pipeline:
    Yahoo ‚Üí CSV ‚Üí Parquet (Staging ‚Üí Core ‚Üí Business ‚Üí Mart) ‚Üí Kafka
    """
    script_path = '/opt/airflow/scripts/main/run_pipeline.py'
    abs_path = os.path.abspath(script_path)
    print(f"üìÇ Absolute script path: {abs_path}")

    if not os.path.exists(abs_path):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y script: {abs_path}")
    
    logging.info(f"‚ñ∂Ô∏è ƒêang ch·∫°y pipeline: {abs_path}")
    try:
        subprocess.run(["python", abs_path], check=True)
        logging.info("‚úÖ Pipeline ch·∫°y th√†nh c√¥ng")
    except subprocess.CalledProcessError as e:
        logging.error(f"‚ùå Pipeline th·∫•t b·∫°i: {e}")
        raise e


# C·∫•u h√¨nh DAG
default_args = {
    'owner': 'trungduc',
    'retries': 1
}

with DAG(
    dag_id='stock_etl_pipeline_mart',
    default_args=default_args,
    description='ETL Pipeline: Yahoo ‚Üí CSV ‚Üí Parquet (Staging ‚Üí Core ‚Üí Business ‚Üí Mart) ‚Üí Kafka',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,  # Ch·∫°y th·ªß c√¥ng
    catchup=False,
    tags=['stock', 'etl', 'pipeline', 'mart']
) as dag:

    run_pipeline_task = PythonOperator(
        task_id='run_main_pipeline',
        python_callable=run_main_pipeline
    )

    run_pipeline_task

