import os
import logging
import subprocess
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime


def run_main_pipeline():
    """
    Gá»i script Python chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ ETL pipeline:
    Yahoo â†’ CSV â†’ Parquet (Staging â†’ Core â†’ Business â†’ Mart) â†’ Superset
    """
    script_path = '/opt/airflow/scripts/main/run_pipeline.py'
    abs_path = os.path.abspath(script_path)
    print(f"ğŸ“‚ Absolute script path: {abs_path}")

    if not os.path.exists(abs_path):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y script: {abs_path}")
    
    logging.info(f"â–¶ï¸ Äang cháº¡y pipeline: {abs_path}")
    try:
        subprocess.run(["python", abs_path], check=True)
        logging.info("âœ… Pipeline cháº¡y thÃ nh cÃ´ng")
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ Pipeline tháº¥t báº¡i: {e}")
        raise e


# Cáº¥u hÃ¬nh DAG
default_args = {
    'owner': 'trungduc',
    'retries': 1
}

with DAG(
    dag_id='aapl_etl_daily',
    default_args=default_args,
    description='ETL Pipeline: Yahoo â†’ CSV â†’ Parquet (Staging â†’ Core â†’ Business â†’ Mart) â†’ Kafka',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['stock', 'etl', 'pipeline', 'mart']
) as dag:

    run_pipeline_task = PythonOperator(
        task_id='run_main_pipeline',
        python_callable=run_main_pipeline
    )

    run_pipeline_task

