# from pyspark.sql import SparkSession

# import os

# from pyspark.sql.window import Window
# from pyspark.sql.functions import lag, when, lit, max as spark_max
# from pyspark.sql.functions import col, to_date, year, month, dayofmonth, weekofyear, round

# def transform_core_to_business(core_path: str, business_path: str):
#     spark = SparkSession.builder.appName("CoreToBusinessStockETL").getOrCreate()

#     try:
#         if not os.path.exists(core_path):
#             raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c core: {core_path}")
#         print(f"üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ Core: {core_path}")

#         df_core = spark.read.parquet(core_path)
#         original_count = df_core.count()
#         print(f"üìä D√≤ng Core: {original_count}")

#         df = df_core.withColumn("Date", to_date(col("Date"), "yyyy-MM-dd"))

#         # L·ªçc gi·ªØ l·∫°i d√≤ng h·ª£p l·ªá nh∆∞ng kh√¥ng lo·∫°i b·ªè qu√° m·∫°nh
#         df = df.filter(col("Date").isNotNull() &
#                        (col("Open") >= 0) & (col("Close") >= 0))

#         df = df.orderBy("Date")
#         window_spec = Window.orderBy("Date")
#         df = df.withColumn("Prev_Close", lag("Close").over(window_spec))

#         df = df \
#             .withColumn("Change", round(col("Close") - col("Open"), 6)) \
#             .withColumn("ChangePercent", round((col("Close") - col("Open")) / col("Open") * 100, 4)) \
#             .withColumn("GapPercent", when(col("Prev_Close").isNotNull(),
#                                            round((col("Close") - col("Prev_Close")) / col("Prev_Close") * 100, 4))
#                         .otherwise(lit(None))) \
#             .withColumn("Year", year(col("Date"))) \
#             .withColumn("Month", month(col("Date"))) \
#             .withColumn("Day", dayofmonth(col("Date"))) \
#             .withColumn("Week", weekofyear(col("Date")))

#         final_count = df.count()
#         print(f"‚úÖ D·ªØ li·ªáu Business: {final_count} d√≤ng (m·∫•t {original_count - final_count} d√≤ng n·∫øu c√≥)")

#         print("üìÜ Kho·∫£ng th·ªùi gian d·ªØ li·ªáu:")
#         df.select(spark_max("Date").alias("Max_Date")).show()
        
#         df.write.mode("overwrite").parquet(business_path)
#         print(f"‚úÖ ƒê√£ ghi d·ªØ li·ªáu Business v√†o: {business_path}")
#         return df

#     except Exception as e:
#         print(f"‚ùå L·ªói Core ‚Üí Business: {e}")
#     finally:
#         spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, to_date, year, month, dayofmonth, weekofyear,
    lag, when, lit, round, max as spark_max
)
from pyspark.sql.window import Window
import os

def transform_core_to_business(core_path: str, business_path: str):
    spark = SparkSession.builder.appName("CoreToBusinessStockETL").getOrCreate()

    try:
        if not os.path.exists(core_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c core: {core_path}")
        print(f"üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ Core: {core_path}")

        df = spark.read.parquet(core_path)
        original_count = df.count()
        print(f"üìä D√≤ng Core: {original_count}")

        if original_count == 0:
            raise ValueError("‚ùå D·ªØ li·ªáu Core r·ªóng.")

        # L√†m s·∫°ch nh·∫π ‚Äì ƒë·∫£m b·∫£o Date v√† Open/Close t·ªìn t·∫°i
        df = df.filter(
            col("Date").isNotNull() &
            col("Open").isNotNull() &
            col("Close").isNotNull()
        )

        # Chuy·ªÉn v·ªÅ th·ª© t·ª± th·ªùi gian
        df = df.orderBy("Date")

        # T√≠nh Prev_Adj_Close n·∫øu thi·∫øu v√† c·ªôt Adj Close c√≥ t·ªìn t·∫°i
        if "Adj Close" in df.columns and "Prev_Adj_Close" not in df.columns:
            df = df.withColumn("Prev_Adj_Close", lag("Adj Close").over(Window.orderBy("Date")))

        # T√≠nh th√™m GapPercent n·∫øu ch∆∞a c√≥ (d·ª±a tr√™n Prev_Adj_Close)
        if "GapPercent" not in df.columns and "Prev_Adj_Close" in df.columns:
            df = df.withColumn("GapPercent", when(
                col("Prev_Adj_Close").isNotNull(),
                round((col("Open") - col("Prev_Adj_Close")) / col("Prev_Adj_Close") * 100, 4)
            ).otherwise(lit(None)))

        # B·ªï sung c√°c tr∆∞·ªùng th·ªùi gian n·∫øu thi·∫øu
        for time_col, func in {
            "Year": year,
            "Month": month,
            "Day": dayofmonth,
            "Week": weekofyear
        }.items():
            if time_col not in df.columns:
                df = df.withColumn(time_col, func(col("Date")))

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        final_count = df.count()
        print(f"‚úÖ D·ªØ li·ªáu Business: {final_count} d√≤ng (kh√¥ng m·∫•t ch·ªâ s·ªë t·ª´ Core)")

        df.select(spark_max("Date").alias("Max_Date")).show()

        # Ghi d·ªØ li·ªáu sang t·∫ßng Business
        df.write.mode("overwrite").parquet(business_path)
        print(f"‚úÖ ƒê√£ ghi d·ªØ li·ªáu Business v√†o: {business_path}")
        return df

    except Exception as e:
        print(f"‚ùå L·ªói Core ‚Üí Business: {e}")
        return None
    finally:
        spark.stop()
