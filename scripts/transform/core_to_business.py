# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, to_date
# import os

# def transform_core_to_business(core_path: str, business_path: str):
#     spark = SparkSession.builder \
#         .appName("CoreToBusinessStockETL") \
#         .getOrCreate()
    
#     try:
#         if not os.path.exists(core_path):
#             raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c core: {core_path}")
        
#         print(f"üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ core: {core_path}")
#         df_core = spark.read.parquet(core_path)

#         if df_core.count() == 0:
#             raise ValueError("‚ùå D·ªØ li·ªáu core r·ªóng.")

#         # Chuy·ªÉn ƒë·ªïi ng√†y n·∫øu c·∫ßn
#         df_core = df_core.withColumn("Date", to_date(col("Date")))

#         # T·ªïng h·ª£p theo ng√†y
#         df_business = df_core.groupBy("Date").agg(
#             {"Open": "first", "High": "max", "Low": "min", "Close": "last", "Volume": "sum"}
#         ).orderBy("Date")

#         # Ghi xu·ªëng Business
#         df_business.write.mode("overwrite").parquet(business_path)
#         print(f"‚úÖ Business data written to: {business_path}")

#         return df_business

#     except Exception as e:
#         print(f"‚ùå L·ªói trong Core ‚Üí Business: {e}")
#     finally:
#         spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, round, lag
from pyspark.sql.window import Window
import os

# def transform_core_to_business(core_path: str, business_path: str):
#     """
#     Chuy·ªÉn d·ªØ li·ªáu t·ª´ t·∫ßng Core sang t·∫ßng Business ƒë·ªÉ ph·ª•c v·ª• ph√¢n t√≠ch BI:
#     - ƒê·∫£m b·∫£o d·ªØ li·ªáu s·∫°ch, ƒë·∫ßy ƒë·ªß
#     - B·ªï sung c√°c ch·ªâ s·ªë ph√¢n t√≠ch nh∆∞ thay ƒë·ªïi gi√°, ph·∫ßn trƒÉm thay ƒë·ªïi

#     Args:
#         core_path (str): ƒê∆∞·ªùng d·∫´n Parquet t·∫ßng Core.
#         business_path (str): ƒê∆∞·ªùng d·∫´n ghi d·ªØ li·ªáu t·∫ßng Business.
#     """
#     # T·∫°o SparkSession
#     spark = SparkSession.builder \
#         .appName("CoreToBusinessStockETL") \
#         .getOrCreate()

#     try:
#         # Ki·ªÉm tra t·ªìn t·∫°i
#         if not os.path.exists(core_path):
#             raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c core: {core_path}")
#         print(f"üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ Core: {core_path}")

#         # ƒê·ªçc d·ªØ li·ªáu parquet t·ª´ core
#         df_core = spark.read.parquet(core_path)

#         if df_core.count() == 0:
#             raise ValueError("‚ùå D·ªØ li·ªáu core r·ªóng.")

#         # Chuy·ªÉn ƒë·ªïi Date sang chu·∫©n datetime n·∫øu c·∫ßn
#         df = df_core.withColumn("Date", to_date(col("Date")))

#         # L·ªçc nh·ªØng d√≤ng null (n·∫øu c√≤n s√≥t l·∫°i)
#         df = df.dropna(subset=["Date", "Open", "Close"])

#         # S·∫Øp x·∫øp theo th·ªùi gian ƒë·ªÉ t√≠nh to√°n ch·ªâ s·ªë
#         df = df.orderBy("Date")

#         # S·ª≠ d·ª•ng window function ƒë·ªÉ l·∫•y gi√° h√¥m tr∆∞·ªõc
#         window_spec = Window.orderBy("Date")
#         df = df.withColumn("Prev_Close", lag("Close").over(window_spec))

#         # T√≠nh c√°c ch·ªâ s·ªë ph√¢n t√≠ch
#         df = df \
#             .withColumn("Change", round(col("Close") - col("Open"), 6)) \
#             .withColumn("ChangePercent", round((col("Close") - col("Open")) / col("Open") * 100, 4)) \
#             .withColumn("GapPercent", round((col("Close") - col("Prev_Close")) / col("Prev_Close") * 100, 4))

#         # In schema v√† preview
#         print("üìä Schema business:")
#         df.printSchema()
#         df.show(5)

#         # Ghi xu·ªëng t·∫ßng business
#         df.write.mode("overwrite").parquet(business_path)
#         print(f"‚úÖ ƒê√£ ghi d·ªØ li·ªáu business v√†o: {business_path}")

#         return df

#     except Exception as e:
#         print(f"‚ùå L·ªói khi x·ª≠ l√Ω Core ‚Üí Business: {e}")
#     finally:
#         spark.stop()


from pyspark.sql.window import Window
from pyspark.sql.functions import lag, when, lit, max as spark_max
from pyspark.sql.functions import col, to_date, year, month, dayofmonth, weekofyear, round

def transform_core_to_business(core_path: str, business_path: str):
    spark = SparkSession.builder.appName("CoreToBusinessStockETL").getOrCreate()

    try:
        if not os.path.exists(core_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c core: {core_path}")
        print(f"üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ Core: {core_path}")

        df_core = spark.read.parquet(core_path)
        original_count = df_core.count()
        print(f"üìä D√≤ng Core: {original_count}")

        df = df_core.withColumn("Date", to_date(col("Date"), "yyyy-MM-dd"))

        # L·ªçc gi·ªØ l·∫°i d√≤ng h·ª£p l·ªá nh∆∞ng kh√¥ng lo·∫°i b·ªè qu√° m·∫°nh
        df = df.filter(col("Date").isNotNull() &
                       (col("Open") >= 0) & (col("Close") >= 0))

        df = df.orderBy("Date")
        window_spec = Window.orderBy("Date")
        df = df.withColumn("Prev_Close", lag("Close").over(window_spec))

        df = df \
            .withColumn("Change", round(col("Close") - col("Open"), 6)) \
            .withColumn("ChangePercent", round((col("Close") - col("Open")) / col("Open") * 100, 4)) \
            .withColumn("GapPercent", when(col("Prev_Close").isNotNull(),
                                           round((col("Close") - col("Prev_Close")) / col("Prev_Close") * 100, 4))
                        .otherwise(lit(None))) \
            .withColumn("Year", year(col("Date"))) \
            .withColumn("Month", month(col("Date"))) \
            .withColumn("Day", dayofmonth(col("Date"))) \
            .withColumn("Week", weekofyear(col("Date")))

        final_count = df.count()
        print(f"‚úÖ D·ªØ li·ªáu Business: {final_count} d√≤ng (m·∫•t {original_count - final_count} d√≤ng n·∫øu c√≥)")

        print("üìÜ Kho·∫£ng th·ªùi gian d·ªØ li·ªáu:")
        df.select(spark_max("Date").alias("Max_Date")).show()

        df.write.mode("overwrite").parquet(business_path)
        print(f"‚úÖ ƒê√£ ghi d·ªØ li·ªáu Business v√†o: {business_path}")
        return df

    except Exception as e:
        print(f"‚ùå L·ªói Core ‚Üí Business: {e}")
    finally:
        spark.stop()
