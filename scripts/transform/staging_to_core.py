# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, to_date
# from pyspark.sql.types import DoubleType
# import os
# def transform_staging_to_core(staging_path: str, core_output_path: str):
#     """
#     ƒê·ªçc d·ªØ li·ªáu t·ª´ t·∫ßng Staging (Parquet), l√†m s·∫°ch v√† chu·∫©n h√≥a, ghi v√†o t·∫ßng Core.

#     Args:
#         staging_path (str): ƒê∆∞·ªùng d·∫´n t·ªõi file Parquet ·ªü t·∫ßng staging.
#         core_output_path (str): ƒê∆∞·ªùng d·∫´n ƒë·ªÉ ghi d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a sang core.
#     """
#     # Kh·ªüi t·∫°o SparkSession
#     spark = SparkSession.builder \
#         .appName("StagingToCoreStockETL") \
#         .getOrCreate()

#     try:
#         if not os.path.exists(staging_path):
#             raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file staging: {staging_path}")
#         print("‚úÖ File staging t·ªìn t·∫°i, b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")

#         # ƒê·ªçc d·ªØ li·ªáu t·ª´ Staging
#         df = spark.read.parquet(staging_path)

#         # L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
#         df_cleaned = df \
#             .withColumn("Date", to_date(col("Date"))) \
#             .withColumn("Open", col("Open").cast(DoubleType())) \
#             .withColumn("High", col("High").cast(DoubleType())) \
#             .withColumn("Low", col("Low").cast(DoubleType())) \
#             .withColumn("Close", col("Close").cast(DoubleType())) \
#             .withColumn("Volume", col("Volume").cast(DoubleType())) \
#             .dropna(subset=["Date", "Open", "Close"]) \
#             .dropDuplicates(["Date"]) \
#             .filter((col("Open") >= 0) & (col("Close") >= 0))

#         # Ghi d·ªØ li·ªáu sang Core
#         df_cleaned.write.mode("overwrite").parquet(core_output_path)
#         print(f"‚úÖ Core data written to: {core_output_path}")

#         return df_cleaned  # Tr·∫£ ra DataFrame n·∫øu c·∫ßn d√πng ti·∫øp

#     except Exception as e:
#         print(f"‚ùå L·ªói khi x·ª≠ l√Ω staging ‚Üí core: {e}")

#     finally:
#         spark.stop()


# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, to_date, year, month, dayofmonth, weekofyear, round
# from pyspark.sql.types import DoubleType
# import os

# def transform_staging_to_core(staging_path: str, core_output_path: str):
#     """
#     Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ t·∫ßng Staging ‚Üí Core:
#     - L√†m s·∫°ch d·ªØ li·ªáu, chu·∫©n h√≥a ki·ªÉu
#     - B·ªï sung c·ªôt ph√¢n t√≠ch th·ªùi gian v√† ch·ªâ s·ªë tƒÉng gi·∫£m gi√°
#     - Ghi ra file Parquet d√πng cho ph√¢n t√≠ch Superset

#     Args:
#         staging_path (str): ƒê∆∞·ªùng d·∫´n file parquet staging.
#         core_output_path (str): ƒê∆∞·ªùng d·∫´n ƒë·ªÉ ghi parquet ƒë√£ chu·∫©n h√≥a.

#     Returns:
#         DataFrame ho·∫∑c None n·∫øu l·ªói
#     """
#     spark = SparkSession.builder.appName("StagingToCoreStockETL").getOrCreate()

#     try:
#         if not os.path.exists(staging_path):
#             raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file staging: {staging_path}")
#         print("üì• ƒêang ƒë·ªçc d·ªØ li·ªáu staging...")

#         # ƒê·ªçc parquet
#         df = spark.read.parquet(staging_path)

#         # In schema staging
#         print("üìú Schema staging:")
#         df.printSchema()

#         # Th·ª≠ count (c√≥ th·ªÉ g√¢y l·ªói n·∫øu file corrupt)
#         try:
#             row_count = df.count()
#             print(f"üîé D·ªØ li·ªáu g·ªëc c√≥ {row_count} d√≤ng")
#         except Exception as err:
#             print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·∫øm d√≤ng staging: {err}")
#             df.show(5)

#         # L√†m s·∫°ch v√† chu·∫©n h√≥a
#         df_cleaned = df \
#             .withColumn("Date", to_date(col("Date"))) \
#             .withColumn("Open", col("Open").cast(DoubleType())) \
#             .withColumn("High", col("High").cast(DoubleType())) \
#             .withColumn("Low", col("Low").cast(DoubleType())) \
#             .withColumn("Close", col("Close").cast(DoubleType())) \
#             .withColumn("Volume", col("Volume").cast(DoubleType())) \
#             .dropna(subset=["Date", "Open", "Close"]) \
#             .dropDuplicates(["Date"]) \
#             .filter((col("Open") >= 0) & (col("Close") >= 0))

#         # B·ªï sung c·ªôt ph·ª•c v·ª• ph√¢n t√≠ch th·ªùi gian
#         df_cleaned = df_cleaned \
#             .withColumn("Year", year(col("Date"))) \
#             .withColumn("Month", month(col("Date"))) \
#             .withColumn("Day", dayofmonth(col("Date"))) \
#             .withColumn("Week", weekofyear(col("Date")))

#         # T√≠nh to√°n bi·∫øn ƒë·ªông gi√°
#         df_cleaned = df_cleaned \
#             .withColumn("Change", round(col("Close") - col("Open"), 6)) \
#             .withColumn("ChangePercent", round((col("Close") - col("Open")) / col("Open") * 100, 4))

#         # In schema sau khi x·ª≠ l√Ω
#         print("üìë Schema sau x·ª≠ l√Ω:")
#         df_cleaned.printSchema()

#         # Ki·ªÉm tra s·ªë d√≤ng sau x·ª≠ l√Ω
#         try:
#             cleaned_count = df_cleaned.count()
#             print(f"‚úÖ D·ªØ li·ªáu s·∫°ch c√≥ {cleaned_count} d√≤ng")
#         except Exception as count_err:
#             print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ count d·ªØ li·ªáu s·∫°ch: {count_err}")
#             df_cleaned.show(5)

#         # Ghi parquet xu·ªëng t·∫ßng Core
#         df_cleaned.write.mode("overwrite").parquet(core_output_path)
#         print(f"üíæ ƒê√£ l∆∞u Core data t·∫°i: {core_output_path}")

#         return df_cleaned

#     except Exception as e:
#         print(f"‚ùå L·ªói khi x·ª≠ l√Ω Staging ‚Üí Core: {e}")
#         return None

#     finally:
#         spark.stop()


# if __name__ == "__main__":
#     # ‚úÖ Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn staging v√† core
#     base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
#     symbol = "AAPL"

#     staging_path = os.path.join(base_dir, f"warehouse/staging/{symbol.lower()}.parquet")
#     core_path = os.path.join(base_dir, f"warehouse/core/{symbol.lower()}_core.parquet")

#     print(f"üß™ B·∫Øt ƒë·∫ßu ki·ªÉm tra Staging ‚Üí Core cho symbol: {symbol}")
#     print(f"üìÇ Staging path: {staging_path}")
#     print(f"üìÇ Core output path: {core_path}")

#     result = transform_staging_to_core(staging_path, core_path)

#     if result is not None:
#         print("‚úÖ [DONE] H√†m transform_staging_to_core ch·∫°y th√†nh c√¥ng.")
#     else:
#         print("‚ùå [FAILED] C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω.")

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, year, month, dayofmonth, weekofyear, round
from pyspark.sql.types import DoubleType
import os

def transform_staging_to_core(staging_path: str, core_output_path: str):
    spark = SparkSession.builder.appName("StagingToCoreStockETL").getOrCreate()

    try:
        if not os.path.exists(staging_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file staging: {staging_path}")
        print("üì• ƒêang ƒë·ªçc d·ªØ li·ªáu staging...")

        df = spark.read.parquet(staging_path)
        df = df.withColumn("Date", to_date(col("Date"), "yyyy-MM-dd"))

        print("üìú Schema staging:")
        df.printSchema()
        original_count = df.count()
        print(f"üî¢ D·ªØ li·ªáu g·ªëc: {original_count} d√≤ng")

        # Chu·∫©n h√≥a ki·ªÉu d·ªØ li·ªáu
        df = df \
            .withColumn("Open", col("Open").cast(DoubleType())) \
            .withColumn("High", col("High").cast(DoubleType())) \
            .withColumn("Low", col("Low").cast(DoubleType())) \
            .withColumn("Close", col("Close").cast(DoubleType())) \
            .withColumn("Volume", col("Volume").cast(DoubleType()))

        # L√†m s·∫°ch m·ªÅm (ch·ªâ lo·∫°i b·ªè d√≤ng c√≥ Date null v√† d·ªØ li·ªáu √¢m)
        df_cleaned = df \
            .filter(col("Date").isNotNull()) \
            .filter((col("Open") >= 0) & (col("Close") >= 0) &
                    (col("High") >= 0) & (col("Low") >= 0) & (col("Volume") >= 0)) \
            .dropDuplicates(["Date"])

        cleaned_count = df_cleaned.count()
        print(f"‚úÖ D·ªØ li·ªáu sau l√†m s·∫°ch: {cleaned_count} d√≤ng (m·∫•t {original_count - cleaned_count})")

        # B·ªï sung ph√¢n t√≠ch th·ªùi gian
        df_cleaned = df_cleaned \
            .withColumn("Year", year(col("Date"))) \
            .withColumn("Month", month(col("Date"))) \
            .withColumn("Day", dayofmonth(col("Date"))) \
            .withColumn("Week", weekofyear(col("Date")))

        # T√≠nh bi·∫øn ƒë·ªông gi√°
        df_cleaned = df_cleaned \
            .withColumn("Change", round(col("Close") - col("Open"), 6)) \
            .withColumn("ChangePercent", round((col("Close") - col("Open")) / col("Open") * 100, 4))

        df_cleaned.write.mode("overwrite").parquet(core_output_path)
        print(f"üíæ ƒê√£ ghi d·ªØ li·ªáu Core t·∫°i: {core_output_path}")
        return df_cleaned

    except Exception as e:
        print(f"‚ùå L·ªói Staging ‚Üí Core: {e}")
        return None
    finally:
        spark.stop()
